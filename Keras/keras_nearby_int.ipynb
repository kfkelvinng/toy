{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting whether integer is nearby (within 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN\n",
    "loss_function = 'binary_crossentropy' # 'categorical_crossentropy' or 'binary_crossentropy'\n",
    "final_activation = 'softmax'          # 'softmax' or 'sigmoid'\n",
    "inner_activation = 'relu'\n",
    "'''Use categorical and softmax if targets are exclusive\n",
    "   Use binary and sigmoid if inputs can have multiple target labels'''\n",
    "\n",
    "dropout_prob = 0.1    # dropout on hidden layer\n",
    "dropout_input = 0.01   # dropout on input features\n",
    "'''Note that dropout often improves performance but also increases training time'''\n",
    "\n",
    "num_nodes = 500       # number of nodes in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NN CODE\n",
    "'''Constructs a simple shallow neural net with dropout.\n",
    "   The training code automatically uses 5% of the training data as a validation set,\n",
    "   and stops training when the loss on the validation set starts to increase, which\n",
    "   reduces the risk of overfitting.\n",
    "   We automatically save the model config to model.json and the model weights to weights.hdf5\n",
    "'''\n",
    "#construct model\n",
    "clf = Sequential()\n",
    "clf.add(Dropout(dropout_input, input_shape=(16,)))\n",
    "clf.add(Dense(num_nodes, input_dim = 16, activation= inner_activation))\n",
    "clf.add(Dropout(dropout_prob))\n",
    "clf.add(Dense(2, activation= final_activation))\n",
    "\n",
    "#compile and train model\n",
    "clf.compile(optimizer='Adam', loss= loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stop if validation get worse\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[random.randint(0,256) for x in range(5000)]\n",
    "b=[random.randint(0,256) for x in range(5000)]\n",
    "c=[[0,1] if abs(xa-xb)<17 else [1,0] for xa,xb in zip(a,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unpackbits(np.array([[1],[2],[3]],dtype=np.uint8), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vec = np.unpackbits(np.array([[xa] for xa in a],dtype=np.uint8),axis=1)\n",
    "b_vec = np.unpackbits(np.array([[xb] for xb in b],dtype=np.uint8),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stack left and right to vector\n",
    ">>> np.hstack([\n",
    "    np.array([[0,0,0,1],[1,0,1,0]]), \n",
    "    np.array([[0,0,0,1],[0,1,1,0]])\n",
    "])\n",
    "\n",
    "array([[0, 0, 0, 1, 0, 0, 0, 1],\n",
    "       [1, 0, 1, 0, 0, 1, 1, 0]])\n",
    "\n",
    "\"\"\"\n",
    "ab_vec = np.hstack([\n",
    "    a_vec,b_vec\n",
    "])\n",
    "c_vec = c\n",
    "\n",
    "train = ab_vec[0:4000]\n",
    "tag_t = c_vec[0:4000]\n",
    "validate = ab_vec[4000:]\n",
    "tag_v = c_vec[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 2s - loss: 0.3888 - val_loss: 0.3504\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.3440 - val_loss: 0.2968\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.2752 - val_loss: 0.2166\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.2148 - val_loss: 0.1716\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1789 - val_loss: 0.1443\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1686 - val_loss: 0.1289\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1577 - val_loss: 0.1154\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1446 - val_loss: 0.1127\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1423 - val_loss: 0.1045\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1308 - val_loss: 0.0976\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1288 - val_loss: 0.1074\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1305 - val_loss: 0.0951\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1173 - val_loss: 0.0836s\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1209 - val_loss: 0.0818\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1149 - val_loss: 0.0759\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1061 - val_loss: 0.0724\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1116 - val_loss: 0.0771\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1104 - val_loss: 0.0701\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1091 - val_loss: 0.0695\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1051 - val_loss: 0.0706\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0977 - val_loss: 0.0663\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0962 - val_loss: 0.0752\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1031 - val_loss: 0.0663\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0916 - val_loss: 0.0580\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0927 - val_loss: 0.0647\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.1002 - val_loss: 0.0633\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0958 - val_loss: 0.0625\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0937 - val_loss: 0.0554\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0857 - val_loss: 0.0567\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0952 - val_loss: 0.0636\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0859 - val_loss: 0.0547\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0847 - val_loss: 0.0523\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0943 - val_loss: 0.0561\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0901 - val_loss: 0.0570\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0989 - val_loss: 0.0631\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0782 - val_loss: 0.0531\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0786 - val_loss: 0.0548\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0919 - val_loss: 0.0509\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0720 - val_loss: 0.0537\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0871 - val_loss: 0.0485\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0848 - val_loss: 0.0476\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0768 - val_loss: 0.0541\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0849 - val_loss: 0.0546\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0731 - val_loss: 0.0489\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0741 - val_loss: 0.0492\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0785 - val_loss: 0.0503\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0771 - val_loss: 0.0578\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0704 - val_loss: 0.0465\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0744 - val_loss: 0.0478\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0737 - val_loss: 0.0520\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0692 - val_loss: 0.0502\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0769 - val_loss: 0.0542\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0744 - val_loss: 0.0506\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0642 - val_loss: 0.0511\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0739 - val_loss: 0.0479\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0738 - val_loss: 0.0523\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0648 - val_loss: 0.0453\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0682 - val_loss: 0.0487\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0662 - val_loss: 0.0483\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0677 - val_loss: 0.0475\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0627 - val_loss: 0.0441\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0687 - val_loss: 0.0464\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0556 - val_loss: 0.0484\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0622 - val_loss: 0.0477\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0695 - val_loss: 0.0500\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0692 - val_loss: 0.0526\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0617 - val_loss: 0.0438\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0648 - val_loss: 0.0528\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0668 - val_loss: 0.0572\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0575 - val_loss: 0.0543\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0638 - val_loss: 0.0483\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0598 - val_loss: 0.0469\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0698 - val_loss: 0.0539\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0717 - val_loss: 0.0508\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0584 - val_loss: 0.0520\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0615 - val_loss: 0.0533\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0708 - val_loss: 0.0581\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 0s - loss: 0.0631 - val_loss: 0.0651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9f4273a860>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train, tag_t, epochs=100, \n",
    "#         class_weight={0:1,1:2}, \n",
    "        validation_data=(validate,tag_v), callbacks=[earlyStopping],\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97199999999999998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "np.mean([x==y for x,y in\n",
    "(zip(np.argmax(clf.predict(validate), axis=1), \n",
    "              np.argmax(tag_v, axis=1)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9568965517241379"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy for [0,1] to validate imbalance class\n",
    "np.mean([x==y for x,y in\n",
    "(zip(np.argmax(clf.predict(validate), axis=1), \n",
    "              np.argmax(tag_v, axis=1)))\n",
    "if y==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.984  0.888]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = np.argmax(tag_v, axis=1)\n",
    "y_pred = np.argmax(clf.predict(validate), axis=1)\n",
    "\n",
    "print(f1_score(y_true, y_pred, average=None))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
