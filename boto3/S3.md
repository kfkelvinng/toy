# S3 Resources (Not recommend...)
High level way to get S3 object
```
s3_resource_bucket = boto3.resource('s3').Bucket('my-bucket')

# List all objects
s3_resource_bucket.objects.all()

# Filter objects by prefix (Filter by Delimiter doesn't show subfolder >_<, use client instead)
s3_resource_bucket.objects.filter(Prefix=my-directory)
```
# S3 Client
Lower level client. Most of the time, this is more useful.
```
import boto3
s3_client = boto3.client('s3',
                    aws_access_key_id=aws_access_key_id,
                    aws_secret_access_key=aws_secret_access_key)

# List S3 immediate subfolders
list_of_s3_objects = s3_client.list_objects_v2(
    Bucket=bucket,
    Prefix=object_key_prefix, Delimiter='/')
subfolders = list_of_s3_objects['CommonPrefix']

# List S3 Object
list_of_s3_objects = s3_client.list_objects_v2(
    Bucket=bucket,
    Prefix=object_key_prefix)
contents = list_of_s3_objects['Contents']

# Download everything
for s3_object in contents:
    s3.download_file(bucket, s3_object['Key'], os.join.path("path",s3_object['Key']))
```


# Verify
```
def etag(filename, chunk_size=None):
  with open(filename, 'rb') as f:
    import hashlib
    if chunk_size:
      md5s = []
      while True:
        data = f.read(chunk_size*1024*1024)
        if not data:
          break
        md5s.append(hashlib.md5(data))
      digests = b"".join(m.digest() for m in md5s)
      return '{}-{}'.format(hashlib.md5(digests).hexdigest(), len(md5s))
    m = hashlib.md5()
    while True:
      data = f.read(10240)
      if len(data) == 0:
          break
      m.update(data)
    return str(m.hexdigest())


def predict_chunk_size(size, chunk_num):
  if chunk_num * 1024 * 1024 * 8 > size > (chunk_num - 1) * 1024 * 1024 * 8:
    yield 8
  chunk_size = ceil(size / 1024 / 1024 / chunk_num)
  while chunk_num * 1024 * 1024 * chunk_size > size > \
          (chunk_num - 1) * 1024 * 1024 * chunk_size:
    yield chunk_size
    chunk_size += 1


def verify_integrity(s3_object, path):
  s3_etag = s3_object['ETag'].replace('"', '')
  if "-" in s3_etag:
    for x in predict_chunk_size(size=s3_object["Size"], chunk_num=int(s3_etag.split("-")[-1])):
      local_etag = etag(path, chunk_size=x)
      if local_etag == s3_etag:
        break
  else:
    local_etag = etag(path)
  print("{} Integrity check {} {}".format(
    "PASS" if s3_etag == local_etag else "FAIL",
    s3_etag, local_etag))
  return s3_etag == local_etag
```


